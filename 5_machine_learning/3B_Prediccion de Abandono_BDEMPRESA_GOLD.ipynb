{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "lastEditStatus": {
   "notebookId": "72t2am2kr4gr44cogkyp",
   "authorId": "5339086794500",
   "authorName": "MGGMUSIC",
   "authorEmail": "mggmusic@gmail.com",
   "sessionId": "878b91b4-a4db-4f81-bf63-2996db79e0fd",
   "lastEditTime": 1737587557719
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "430ebdf6-dab3-4096-aec8-26c9fe17f603",
   "metadata": {
    "collapsed": false,
    "name": "cell1",
    "resultHeight": 360
   },
   "source": "# :chart_with_upwards_trend: Modelo de abandono de clientes: Ingeniería de características (Features)\n\n### Ejecute el primer Notebook por completo antes de ejecutar este segundo Notebook.\n\n### Primero, agregue los paquetes `imbalanced-learn`, `snowflake-ml-python`, `altair`, `pandas` y `numpy` desde el selector de paquetes en la parte superior derecha. Usaremos estos paquetes más adelante en el Notebook.\n\nPara preparar nuestros datos para nuestro modelo, necesitaremos manejar el problema de datos desequilibrados mediante un muestreo ascendente de nuestro conjunto de datos.\n\nPara esto, usaremos el algoritmo `SMOTE` del paquete `imblearn`."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "language": "python",
    "name": "cell2"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "import altair as alt\n",
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n",
    "session.query_tag = {\"origin\":\"sf_sit-is\", \n",
    "                     \"name\":\"churn_prediction\", \n",
    "                     \"version\":{\"major\":1, \"minor\":0},\n",
    "                     \"attributes\":{\"is_quickstart\":1, \"source\":\"notebook\"}}\n",
    "\n",
    "# Saving telco_churn_pdf into variable from Snowflake\n",
    "telco_churn_pdf = session.sql(\"SELECT * FROM TELCO_CHURN_PDF\").to_pandas()\n",
    "\n",
    "# Extract the training features\n",
    "features_names = [col for col in telco_churn_pdf.columns if col not in ['Churn']]\n",
    "features = telco_churn_pdf[features_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7e170c-32e8-4902-abf8-fd8e09de6fa4",
   "metadata": {
    "name": "cell3",
    "language": "python",
    "resultHeight": 654
   },
   "outputs": [],
   "source": "# extract the target\ntarget = telco_churn_pdf['Churn']\nst.markdown(\"## Vamos a equilibrar el conjunto de datos.\")\n# upsample the minority class in the dataset\nupsampler = SMOTE(random_state = 111)\nfeatures, target = upsampler.fit_resample(features, target)\nst.dataframe(features.head())\n\nst.markdown(\"## Upsampled data.\")\nupsampled_data = pd.concat([features, target], axis=1)\nupsampled_data.reset_index(inplace=True)\nupsampled_data.rename(columns={'index': 'INDEX'}, inplace=True)\nst.dataframe(upsampled_data.head())\n\nst.markdown(\"## Preview of upsampled data.\")\nupsampled_data = session.create_dataframe(upsampled_data)\n# Get the list of column names from the dataset\nfeature_names_input = [c for c in upsampled_data.columns if c != '\"Churn\"' and c != \"INDEX\"]\nupsampled_data[feature_names_input]"
  },
  {
   "cell_type": "markdown",
   "id": "9a1f5f7f-777a-4f26-82cc-213b6b3c04e0",
   "metadata": {
    "collapsed": false,
    "name": "cell4",
    "resultHeight": 220
   },
   "source": "Una vez que nos hayamos ocupado de eso, usaremos scikit-learn para preprocesar nuestros datos en un formato que el modelo espera. Esto significa escalar nuestras características y dividir nuestros datos en conjuntos de datos de prueba y entrenamiento.\n\nPodemos realizar el preprocesamiento de StandardScaler a través de sklearn para procesar en memoria o el preprocesamiento de Snowpark ML para el procesamiento pushdown.\n\n## Preprocesamiento de Sci-kit learn con Pandas DataFrames"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9533336-f5d3-461c-aa75-b604e23e44c1",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "cell5",
    "resultHeight": 439
   },
   "outputs": [],
   "source": [
    "import sklearn.preprocessing as pp_original\n",
    "# Initialize a StandardScaler object with input and output column names\n",
    "scaler = pp_original.StandardScaler()\n",
    "features_pdf = upsampled_data[feature_names_input].to_pandas()\n",
    "\n",
    "# Fit the scaler to the dataset\n",
    "scaler.fit(features_pdf)\n",
    "\n",
    "# Transform the dataset using the fitted scaler\n",
    "scaled_features = scaler.transform(features_pdf)\n",
    "scaled_features = pd.DataFrame(scaled_features, columns = features_names)\n",
    "scaled_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a35603-423c-4da8-9e4c-cb9d77c2e081",
   "metadata": {
    "collapsed": false,
    "name": "cell6",
    "resultHeight": 102
   },
   "source": "## Preprocesamiento de Snowpark ML con Snowpark\n\nTenga en cuenta la similitud entre las API utilizadas para sklearn y Snowpark ML."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa37e57b-9410-47d8-9d88-d3d3438f3469",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "cell7",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "import snowflake.ml.modeling.preprocessing as pp\n",
    "\n",
    "# Initialize a StandardScaler object with input and output column names\n",
    "scaler = pp.StandardScaler(\n",
    "    input_cols=feature_names_input,\n",
    "    output_cols=feature_names_input\n",
    ")\n",
    "\n",
    "# Fit the scaler to the dataset\n",
    "scaler.fit(upsampled_data)\n",
    "\n",
    "# Transform the dataset using the fitted scaler\n",
    "scaled_features = scaler.transform(upsampled_data)\n",
    "scaled_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410681cf-c3b1-47dd-9b58-85c2b2afbc8f",
   "metadata": {
    "collapsed": false,
    "name": "cell8",
    "resultHeight": 89
   },
   "source": "## Realicemos la prueba de división de los datos de prueba utilizando 80/20."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589337dc-d674-40d6-9b73-a9d35fa28086",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "cell9",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# Split the scaled_features dataset into training and testing sets with an 80/20 ratio\n",
    "training, testing = scaled_features.random_split(weights=[0.8, 0.2], seed=111)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd84cf31-4ae6-4f9b-a805-cad7f0d1c38e",
   "metadata": {
    "collapsed": false,
    "name": "cell10",
    "resultHeight": 205
   },
   "source": "# Entrenamiento de modelos: Random Forest Classifier \n\nEl modelo del día es un [Random Forest Classifier ](https://towardsdatascience.com/understanding-random-forest-58381e0602d2). No entraré en detalles sobre cómo funciona, pero, en resumen, crea un conjunto de modelos más pequeños que hacen predicciones sobre los mismos datos. La predicción que tenga más votos será la predicción final con la que se decida el modelo."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15803558-604a-4312-b838-46c71ec74bf3",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "cell11",
    "resultHeight": 439
   },
   "outputs": [],
   "source": [
    "from snowflake.ml.modeling.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the target variable (label) column name\n",
    "label = ['\"Churn\"']\n",
    "\n",
    "# Define the output column name for the predicted label\n",
    "output_label = ['\"predicted_churn\"']\n",
    "\n",
    "# Initialize a RandomForestClassifier object with input, label, and output column names\n",
    "model = RandomForestClassifier(\n",
    "    input_cols=feature_names_input,\n",
    "    label_cols=label,\n",
    "    output_cols=output_label,\n",
    ")\n",
    "\n",
    "# Train the RandomForestClassifier model using the training set\n",
    "_ = model.fit(training)\n",
    "\n",
    "# Predict the target variable (churn) for the testing set using the trained model\n",
    "results = model.predict(testing)\n",
    "\n",
    "testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701cf188-e74f-4de6-8f56-1569879ac2d4",
   "metadata": {
    "collapsed": false,
    "name": "cell12",
    "resultHeight": 141
   },
   "source": "# Evaluación del modelo\n\nLa evaluación del modelo consiste en comprobar el rendimiento de nuestro modelo de aprendizaje automático comparando sus predicciones con los resultados reales."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc8f6c8-5d53-4608-a56d-e0e959a27bd0",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "cell13",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# return only the predicted churn values\n",
    "predictions = results.to_pandas().sort_values(\"INDEX\")[output_label].astype(int).to_numpy().flatten()\n",
    "actual = testing.to_pandas().sort_values(\"INDEX\")[['Churn']].to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fd435b-b357-4d86-bc00-976884b9d590",
   "metadata": {
    "collapsed": false,
    "name": "cell14",
    "resultHeight": 179
   },
   "source": "## Importancia de las características (Features)\n\nLa importancia de las características consiste en determinar qué variables de entrada son las verdaderas MVP a la hora de hacer predicciones con nuestro modelo de aprendizaje automático. Descubriremos qué características son las más importantes observando cuánto contribuyen al rendimiento general del modelo."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418c65c8-c6f2-4a63-b8cd-9c139a395a11",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "language": "python",
    "name": "cell15",
    "resultHeight": 300
   },
   "outputs": [],
   "source": [
    "rf = model.to_sklearn()\n",
    "importances = pd.DataFrame(\n",
    "    list(zip(features.columns, rf.feature_importances_)),\n",
    "    columns=[\"feature\", \"importance\"],\n",
    ")\n",
    "\n",
    "bar_chart = alt.Chart(importances).mark_bar().encode(\n",
    "    x=\"importance:Q\",\n",
    "    y=alt.Y(\"feature:N\", sort=\"-x\")\n",
    ")\n",
    "st.altair_chart(bar_chart, use_container_width=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65609c1c-8225-48eb-a6c0-273e837a6b37",
   "metadata": {
    "name": "cell16",
    "resultHeight": 127,
    "collapsed": false
   },
   "source": "## Predicción de la pérdida de un nuevo usuario\n\nAl utilizar nuestro modelo de bosque aleatorio entrenado, podemos hacer predicciones que nos indiquen si un nuevo cliente se dará de baja o no."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acc1477-d2b3-42f8-94be-4757ae9bc1e8",
   "metadata": {
    "name": "cell17",
    "language": "python",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "account_weeks = \"10\"\n",
    "data_usage = \"1.7\"\n",
    "mins_per_month = \"82\"\n",
    "daytime_calls = \"67\"\n",
    "customer_service_calls = \"4\"\n",
    "monthly_charge = \"37\"\n",
    "roam_mins = \"0\"\n",
    "overage_fee = \"9.5\"\n",
    "renewed_contract = \"true\"\n",
    "has_data_plan = \"true\"\n",
    "user_vector = np.array([\n",
    "    account_weeks,\n",
    "    1 if renewed_contract else 0,\n",
    "    1 if has_data_plan else 0,\n",
    "    data_usage,\n",
    "    customer_service_calls,\n",
    "    mins_per_month,\n",
    "    daytime_calls,\n",
    "    monthly_charge,\n",
    "    overage_fee,\n",
    "    roam_mins,\n",
    "]).reshape(1,-1)\n",
    "\n",
    "user_dataframe = pd.DataFrame(user_vector, columns=[f'\"{_}\"' for _ in features.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddaaeba-7b81-48e9-862a-3f18c849b51e",
   "metadata": {
    "name": "cell18",
    "resultHeight": 47,
    "collapsed": false
   },
   "source": "#### Marco de datos de entrada para el nuevo usuario"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9dc866-600a-4fd7-932b-61ef7195ab83",
   "metadata": {
    "name": "cell19",
    "language": "python",
    "resultHeight": 286
   },
   "outputs": [],
   "source": [
    "st.markdown(\"#### New user\")\n",
    "user_dataframe\n",
    "user_vector = scaler.transform(user_dataframe)\n",
    "st.markdown(\"#### Churn prediction\")\n",
    "model.predict(user_vector)[['\"predicted_churn\"']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1d4a12-038c-43bc-bb20-0647c4d16cb6",
   "metadata": {
    "name": "cell20",
    "language": "python",
    "resultHeight": 232
   },
   "outputs": [],
   "source": [
    "st.markdown(\"#### Scaled dataframe for new user\")\n",
    "st.dataframe(user_vector)\n",
    "st.markdown(\"#### Prediction\")\n",
    "predicted_value = model.predict(user_vector)[['\"predicted_churn\"']].values.astype(int).flatten()\n",
    "user_probability = model.predict_proba(user_vector)\n",
    "probability_of_prediction = max(user_probability[user_probability.columns[-2:]].values[0]) * 100\n",
    "prediction = 'churn' if predicted_value == 1 else 'not churn'\n",
    "st.markdown(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90346f8e-dc24-4f39-b4f0-e6eb385329da",
   "metadata": {
    "name": "cell21",
    "language": "python",
    "resultHeight": 709
   },
   "outputs": [],
   "source": "col1, col2 = st.columns(2)\n\nwith col1: \n    account_weeks = st.slider(\"Semanas como cliente\", int(features[\"AccountWeeks\"].min()) , int(features[\"AccountWeeks\"].max()))\n    data_usage = st.slider(\"Minutos Diarios\", int(features[\"DataUsage\"].min()) , int(features[\"DataUsage\"].max()))\n    mins_per_month = st.slider(\"Minutos Mensuales\", int(features[\"DayMins\"].min()) , int(features[\"DayMins\"].max()))\n    daytime_calls = st.slider(\"Interacciones Diarias\", int(features[\"DayCalls\"].min()) , int(features[\"DayCalls\"].max()))\n    #renewed_contract =  st.selectbox(\"Renovación contrato?\",('true','false'))\n    \nwith col2: \n    monthly_charge = st.slider(\"Cargo recurrente\", int(features[\"MonthlyCharge\"].min()) , int(features[\"MonthlyCharge\"].max()))\n    roam_mins = st.slider(\"Antiguedad\", int(features[\"RoamMins\"].min()) , int(features[\"RoamMins\"].max()))\n    customer_service_calls = st.slider(\"Contactos a soporte\", int(features[\"CustServCalls\"].min()) , int(features[\"CustServCalls\"].max()))\n    overage_fee = st.slider(\"Cargos adicionales\", int(features[\"OverageFee\"].min()) , int(features[\"OverageFee\"].max()))\n    #has_data_plan = st.selectbox(\"Tiene cuenta Premium?\",('true','false'))\n\nuser_vector = np.array([\n    account_weeks,\n    1 if renewed_contract else 0,\n    1 if has_data_plan else 0,\n    data_usage,\n    customer_service_calls,\n    mins_per_month,\n    daytime_calls,\n    monthly_charge,\n    overage_fee,\n    roam_mins,\n]).reshape(1,-1)\n\nuser_dataframe = pd.DataFrame(user_vector, columns=[f'\"{_}\"' for _ in features.columns])\nuser_vector = scaler.transform(user_dataframe)\nwith col1: \n    st.markdown(\"#### Input dataframe for new user\")\n    st.dataframe(user_dataframe)\nwith col2:\n    st.markdown(\"#### Scaled dataframe for new user\")\n    st.dataframe(user_vector)\n\nst.markdown(\"#### Prediction\")\npredicted_value = model.predict(user_vector)[['\"predicted_churn\"']].values.astype(int).flatten()\nuser_probability = model.predict_proba(user_vector)\nprobability_of_prediction = max(user_probability[user_probability.columns[-2:]].values[0]) * 100\nprediction = 'churn' if predicted_value == 1 else 'not churn'\nst.markdown(prediction)"
  },
  {
   "cell_type": "markdown",
   "id": "7834c376-a20b-428d-ab7d-418fd6550556",
   "metadata": {
    "name": "cell22",
    "resultHeight": 60
   },
   "source": [
    "## Exporting Model with Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2756144a-6097-4c2a-bf20-00485b87478d",
   "metadata": {
    "name": "cell23",
    "language": "python",
    "resultHeight": 38
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import datetime\n",
    "filename = f'telco-eda-model-{datetime.datetime.now()}.pkl'\n",
    "\n",
    "pickle.dump(model, open(filename,'wb'))\n",
    "print(f\"Saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10fb5bb-0183-463a-bc4e-7bbc039fc91c",
   "metadata": {
    "name": "cell24",
    "resultHeight": 92
   },
   "source": [
    "Congratulations on making it to the end of this Lab where we explored churn modeling using Snowflake Notebooks! We learned how to import/load data to Snowflake, train a Random Forest model, visualize predictions, and build an interactive data app, and make predictions for new users.\n"
   ]
  }
 ]
}
